{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "spot = pd.read_csv('./data/spot/clarkson_data.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "pmx_forw = pd.read_csv('./data/ffa/PMAX_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "csz_forw = pd.read_csv('./data/ffa/CSZ_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "smx_forw = pd.read_csv('./data/ffa/SMX_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "#oecd_ip_dev = pd.read_csv('./data/other/oecd_daily.csv', parse_dates=['Date'], dayfirst=True)\n",
    "#fleet_dev = pd.read_csv('./data/other/fleet_dev_daily.csv', parse_dates=['Date'], dayfirst=True)\n",
    "eur_usd = pd.read_csv('./data/other/EUR_USD_historical.csv', parse_dates=['Date'], delimiter=\";\", dayfirst=True)\n",
    "# Convert 'Last' column to numeric, replacing comma with dot for decimal point\n",
    "eur_usd['Last'] = pd.to_numeric(eur_usd['Last'].str.replace(',', '.'), errors='coerce')\n",
    "\n",
    "\n",
    "def pick_forw(key):\n",
    "    if key == \"PMX\":\n",
    "        return pmx_forw\n",
    "    elif key == \"CSZ\":\n",
    "        return csz_forw\n",
    "    elif key == \"SMX\":\n",
    "        return smx_forw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3597\n",
      "Round 1\n",
      "2880\n",
      "Epoch 1/2\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0171\n",
      "Epoch 2/2\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step - loss: 0.0018\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Test Score spot MLP: 0.01784 MSE\n",
      "Test Score forw MLP: 0.00713 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0084\n",
      "Epoch 2/2\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0020\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "LSTM Test Score spot: 0.00107 MSE\n",
      "LSTM Test Score forw: 0.00470 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2870/2870\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0065\n",
      "Epoch 2/2\n",
      "\u001b[1m2870/2870\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2869/2869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0209\n",
      "Epoch 2/2\n",
      "\u001b[1m2869/2869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0019\n",
      "WARNING:tensorflow:5 out of the last 94 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x28b191c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0096\n",
      "Epoch 2/2\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0024\n",
      "WARNING:tensorflow:6 out of the last 95 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x28b2485e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "MLP POINT Test Score spot: 0.00442 MSE\n",
      "MLP POINT Test Score forw: 0.01099 MSE\n",
      "Round 2\n",
      "2883\n",
      "Epoch 1/2\n",
      "\u001b[1m2871/2871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0108\n",
      "Epoch 2/2\n",
      "\u001b[1m2871/2871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284us/step - loss: 0.0016\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Test Score spot MLP: 0.00802 MSE\n",
      "Test Score forw MLP: 0.05386 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2871/2871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0105\n",
      "Epoch 2/2\n",
      "\u001b[1m2871/2871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0020\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "LSTM Test Score spot: 0.01110 MSE\n",
      "LSTM Test Score forw: 0.10349 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2873/2873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0046\n",
      "Epoch 2/2\n",
      "\u001b[1m2873/2873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271us/step - loss: 0.0012\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2872/2872\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0209\n",
      "Epoch 2/2\n",
      "\u001b[1m2872/2872\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0021\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2871/2871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299us/step - loss: 0.0127\n",
      "Epoch 2/2\n",
      "\u001b[1m2871/2871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0026\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "MLP POINT Test Score spot: 0.00451 MSE\n",
      "MLP POINT Test Score forw: 0.02881 MSE\n",
      "Round 3\n",
      "2886\n",
      "Epoch 1/2\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289us/step - loss: 0.0550\n",
      "Epoch 2/2\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276us/step - loss: 0.0020\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Test Score spot MLP: 0.00936 MSE\n",
      "Test Score forw MLP: 0.01036 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0144\n",
      "Epoch 2/2\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0020\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "LSTM Test Score spot: 0.00138 MSE\n",
      "LSTM Test Score forw: 0.01683 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0096\n",
      "Epoch 2/2\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0010\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292us/step - loss: 0.0180\n",
      "Epoch 2/2\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0019\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290us/step - loss: 0.0080\n",
      "Epoch 2/2\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0024\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "MLP POINT Test Score spot: 0.02678 MSE\n",
      "MLP POINT Test Score forw: 0.01211 MSE\n",
      "Round 4\n",
      "2889\n",
      "Epoch 1/2\n",
      "\u001b[1m2877/2877\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291us/step - loss: 0.0224\n",
      "Epoch 2/2\n",
      "\u001b[1m2877/2877\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0018\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Test Score spot MLP: 0.00639 MSE\n",
      "Test Score forw MLP: 0.00793 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2877/2877\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0084\n",
      "Epoch 2/2\n",
      "\u001b[1m2877/2877\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0017\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "LSTM Test Score spot: 0.00753 MSE\n",
      "LSTM Test Score forw: 0.00136 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2879/2879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288us/step - loss: 0.0069\n",
      "Epoch 2/2\n",
      "\u001b[1m2879/2879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276us/step - loss: 0.0011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2878/2878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0037\n",
      "Epoch 2/2\n",
      "\u001b[1m2878/2878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0017\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2877/2877\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279us/step - loss: 0.0093\n",
      "Epoch 2/2\n",
      "\u001b[1m2877/2877\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280us/step - loss: 0.0025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "MLP POINT Test Score spot: 0.00737 MSE\n",
      "MLP POINT Test Score forw: 0.02299 MSE\n",
      "Round 5\n",
      "2892\n",
      "Epoch 1/2\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282us/step - loss: 0.0196\n",
      "Epoch 2/2\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0020\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Test Score spot MLP: 0.01709 MSE\n",
      "Test Score forw MLP: 0.00171 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0091\n",
      "Epoch 2/2\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0017\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "LSTM Test Score spot: 0.00175 MSE\n",
      "LSTM Test Score forw: 0.01990 MSE\n",
      "Epoch 1/2\n",
      "\u001b[1m2882/2882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280us/step - loss: 0.0066\n",
      "Epoch 2/2\n",
      "\u001b[1m2882/2882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289us/step - loss: 0.0012\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2881/2881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281us/step - loss: 0.0288\n",
      "Epoch 2/2\n",
      "\u001b[1m2881/2881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287us/step - loss: 0.0019\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279us/step - loss: 0.0153\n",
      "Epoch 2/2\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0022\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "MLP POINT Test Score spot: 0.01650 MSE\n",
      "MLP POINT Test Score forw: 0.01496 MSE\n"
     ]
    }
   ],
   "source": [
    "# Number of rounds based on the test set size and forecast horizon\n",
    "num_rounds =  5  # Adjusted to ensure we don't exceed the test set\n",
    "look_back = 10  # Adjust based on your temporal structure\n",
    "hor = 3\n",
    "scaler = MinMaxScaler()\n",
    "s_col = \"CSZ\"\n",
    "f_col = \"1MON\"\n",
    "#fleet_col = \"CSZ fleet\"\n",
    "forw = pick_forw(s_col)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure 'Date' columns are in datetime format for all datasets\n",
    "#oecd_ip_dev['Date'] = pd.to_datetime(oecd_ip_dev['Date'])\n",
    "#fleet_dev['Date'] = pd.to_datetime(fleet_dev['Date'])\n",
    "eur_usd['Date'] = pd.to_datetime(eur_usd['Date'])\n",
    "spot['Date'] = pd.to_datetime(spot['Date'])\n",
    "pmx_forw['Date'] = pd.to_datetime(pmx_forw['Date'])\n",
    "csz_forw['Date'] = pd.to_datetime(csz_forw['Date'])\n",
    "smx_forw['Date'] = pd.to_datetime(smx_forw['Date'])\n",
    "\n",
    "\n",
    "#prod_col = 'Ind Prod Excl Const VOLA'\n",
    "eur_col = 'Last'\n",
    "\n",
    "# Merge data frames on the Date column\n",
    "data_combined = pd.merge(spot, forw, on='Date')\n",
    "#data_combined = pd.merge(data_combined, oecd_ip_dev[['Date', prod_col]], on='Date', how='inner')\n",
    "#data_combined = pd.merge(data_combined, fleet_dev[['Date', fleet_col]], on='Date', how='inner')\n",
    "data_combined = pd.merge(data_combined, eur_usd[['Date', eur_col]], on='Date', how='inner')\n",
    "\n",
    "\n",
    "# Filter out rows where the specified columns contain zeros or NA values\n",
    "cols_to_check = [s_col, f_col, eur_col]\n",
    "data_combined = data_combined.dropna(subset=cols_to_check)  # Drop rows where NA values are present in the specified columns\n",
    "data_combined = data_combined[(data_combined[cols_to_check] != 0).all(axis=1)]  # Drop rows where 0 values are present in the specified columns\n",
    "\n",
    "\n",
    "# Remove rows with NA or 0 in specific columns (assuming 'SMX' and '1Q' are column names in 'data_combined')\n",
    "#data_combined = data_combined[(data_combined[s_col].notna() & data_combined[s_col] != 0) & (data_combined[f_col].notna() & data_combined[f_col] != 0)]\n",
    "\n",
    "# Transform data to log levels\n",
    "data_log_levels = pd.DataFrame()\n",
    "data_log_levels[\"spot\"] = np.log(data_combined[s_col])\n",
    "data_log_levels[\"forwp\"] = np.log(data_combined[f_col])\n",
    "#data_log_levels[fleet_col] = np.log(data_combined[fleet_col])\n",
    "#data_log_levels[prod_col] = np.log(data_combined[prod_col])\n",
    "data_log_levels[eur_col] = np.log(data_combined[eur_col])\n",
    "\n",
    "data_log_levels.index = data_combined[\"Date\"]\n",
    "\n",
    "split_index = math.floor(len(data_log_levels) * 0.8)\n",
    "print(len(data_log_levels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize dictionary to store MSE results for each model\n",
    "mse_results = {\n",
    "    'MLP_spot': [],\n",
    "    'MLP_forwp': [],\n",
    "    'MLP_POINT_spot': [],\n",
    "    'MLP_POINT_forwp': [],    \n",
    "    'LSTM_spot': [],\n",
    "    'LSTM_forwp': [],\n",
    "    'RW_spot': [],\n",
    "    'RW_forwp': [],\n",
    "}\n",
    "\n",
    "def random_walk_predictions(training_data, testing_data):\n",
    "    \"\"\"\n",
    "    Generates Random Walk predictions where the next value is assumed to be the last observed value.\n",
    "    \n",
    "    Parameters:\n",
    "    - training_data: DataFrame containing the training data.\n",
    "    - testing_data: DataFrame containing the test data.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: Numpy array containing Random Walk predictions for the test set.\n",
    "    \"\"\"\n",
    "    # Last observed values from the training set\n",
    "    last_observed_spot = training_data['spot'].iloc[-1]\n",
    "    last_observed_forwp = training_data['forwp'].iloc[-1]\n",
    "    \n",
    "    # Create an array of predictions, each one equal to the last observed values\n",
    "    predictions = np.tile([last_observed_spot, last_observed_forwp], (len(testing_data), 1))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def create_even_odd_array(arr):\n",
    "    \"\"\"\n",
    "    Returns an array where the first column contains values from even positions\n",
    "    and the second column contains values from odd positions of the original array.\n",
    "    \"\"\"\n",
    "    return arr.reshape(-1, 2)\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=10, hor=1, is_test=False, exog_col=None):\n",
    "    X, Y = [], []\n",
    "    if is_test:  # for test data, we just need the last entry for 1-step ahead forecast\n",
    "        X = dataset[-1:,:,]\n",
    "        return np.array(X), None\n",
    "    else:\n",
    "        for i in range(look_back, len(dataset) - hor + 1):\n",
    "            X.append(dataset[i - look_back:i])\n",
    "            if exog_col is not None:\n",
    "                # Exclude specified columns from Y\n",
    "                y = np.delete(dataset[i:i + hor], exog_col, axis=1)\n",
    "            else:\n",
    "                y = dataset[i:i + hor]\n",
    "            Y.append(y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def create_dataset_point(dataset, look_back=10, hor=1, is_test=False, exog_col=None):\n",
    "    X, Y = [], []\n",
    "    if is_test:  # for test data, we just need the last entry for 1-step ahead forecast\n",
    "        X = dataset[-1:,:,:]\n",
    "        return np.array(X), None\n",
    "    else:\n",
    "        for i in range(look_back, len(dataset) - hor + 1):\n",
    "            X.append(dataset[i - look_back:i])\n",
    "            if exog_col is not None:\n",
    "                # Exclude specified columns and take only the point at 'hor' for Y\n",
    "                y = np.delete(dataset[i + hor - 1: i + hor], exog_col, axis=1)\n",
    "            else:\n",
    "                y = dataset[i + hor - 1: i + hor]\n",
    "            Y.append(y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "# Adjust train and test sets for each forecast round\n",
    "for round in range(1, num_rounds + 1):\n",
    "    print(\"Round\", round)\n",
    "    # Define new split point for each round\n",
    "    split_index = split_index +  hor\n",
    "    print(split_index)\n",
    "    \n",
    "    # Update train and test sets\n",
    "    train = data_log_levels.iloc[:split_index]\n",
    "    test = data_log_levels.iloc[split_index:split_index+hor]\n",
    "\n",
    "    #Scale train set\n",
    "    train_scal = scaler.fit_transform(train)\n",
    "\n",
    "    trainX, trainY = create_dataset(train_scal, look_back=look_back, hor=hor, exog_col=[2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create and fit the MLP model\n",
    "\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    trainY_flat = trainY.reshape(trainY.shape[0], -1)\n",
    "\n",
    "    model_mlp = Sequential()\n",
    "    model_mlp.add(Input(shape=(trainX_flat.shape[1],)))  # Add Input layer\n",
    "    model_mlp.add(Dense(32, activation='relu'))\n",
    "    model_mlp.add(Dense(trainY_flat.shape[1], activation=\"linear\"))\n",
    "    model_mlp.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_mlp.fit(trainX_flat, trainY_flat, epochs=2, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "    # Make predictions\n",
    "    trainPredict_scal_flat = model_mlp.predict(trainX_flat)\n",
    "    testX = train_scal[-look_back:].reshape(1, look_back, train_scal.shape[1])\n",
    "    #testX, _ = create_dataset(trainX, look_back=look_back, is_test=True)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "    testPredict_scal_flat = model_mlp.predict(testX_flat)\n",
    "    testPredict_scal = create_even_odd_array(testPredict_scal_flat)\n",
    "\n",
    "    # Invert predictions\n",
    "    #testPredict_mlp = scaler.inverse_transform(testPredict_scal)\n",
    "    placeholder_exog = np.zeros((testPredict_scal.shape[0], 1))  # Assuming zero as placeholder\n",
    "    testPredict_expanded = np.hstack((testPredict_scal, placeholder_exog))\n",
    "    # Step 2: Inverse transform the expanded array\n",
    "    testPredict_mlp_expanded = scaler.inverse_transform(testPredict_expanded)\n",
    "    # Step 3: Extract the original predictions (now inversely scaled)\n",
    "    testPredict_mlp = testPredict_mlp_expanded[:, :2]  # Assuming the first two columns are what you need\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    testScore = mean_squared_error(test[\"spot\"], testPredict_mlp[:,0])\n",
    "    testScoreForw = mean_squared_error(test[\"forwp\"], testPredict_mlp[:,1])\n",
    "    print('Test Score spot MLP: %.5f MSE' % (testScore))\n",
    "    print('Test Score forw MLP: %.5f MSE' % (testScoreForw))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define the LSTM model\n",
    "    #model_lstm = Sequential()\n",
    "    #model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    #model_lstm.add(LSTM(units=50))\n",
    "    #model_lstm.add(Dense(trainY_flat.shape[1], activation='linear'))  # Assuming multi-step forecasting\n",
    "\n",
    "    #model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    #model_lstm.fit(trainX, trainY_flat, epochs=30, batch_size=1, verbose=0)\n",
    "    \n",
    "\n",
    "\n",
    "    # Define the LSTM model with the Input layer\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(Input(shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model_lstm.add(LSTM(units=50, return_sequences=True))\n",
    "    model_lstm.add(LSTM(units=50))\n",
    "    model_lstm.add(Dense(trainY_flat.shape[1], activation='linear'))\n",
    "\n",
    "    model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_lstm.fit(trainX, trainY_flat, epochs=2, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Prepare the last sequence from the training set as the input for the first prediction\n",
    "    #testX_last_sequence = train_scal[-look_back:].reshape(1, look_back, train_scal.shape[1])\n",
    "\n",
    "    # Make predictions\n",
    "    testPredict_scal_flat = model_lstm.predict(testX)\n",
    "\n",
    "    # Since you're predicting `hor` steps ahead, you might need to adjust the code to generate\n",
    "    # multiple steps if your LSTM model is set up for single-step predictions.\n",
    "    # For simplicity, this example directly uses the LSTM output for multi-step predictions.\n",
    "\n",
    "    # Invert scaling\n",
    "    testPredict_scal = create_even_odd_array(testPredict_scal_flat)\n",
    "    # Expand the array to include the exogenous column for inverse scaling\n",
    "    # Here, placeholder_exog needs to be reshaped or sliced appropriately to match the testPredict_scal's number of rows\n",
    "    placeholder_exog = np.zeros((testPredict_scal.shape[0], 1))  # Example placeholder\n",
    "\n",
    "    # Combine LSTM predictions with the placeholder for the exogenous variable\n",
    "    testPredict_expanded = np.hstack((testPredict_scal, placeholder_exog))\n",
    "\n",
    "    # Inverse transform the expanded array to scale back to original scale\n",
    "    testPredict_lstm_expanded = scaler.inverse_transform(testPredict_expanded)\n",
    "\n",
    "    # Extract only the inversely scaled predictions, excluding the exogenous variable's placeholder column\n",
    "    testPredict_lstm = testPredict_lstm_expanded[:, :2]  # Assuming the first two columns are the endogenous variables you predicted\n",
    "    #testPredict_lstm = scaler.inverse_transform(testPredict_scal)\n",
    "\n",
    "\n",
    "    # Calculate and print MSE for each target\n",
    "    testScore_spot_lstm = mean_squared_error(test[\"spot\"].iloc[:hor].values, testPredict_lstm[:,0])\n",
    "    testScore_forw_lstm = mean_squared_error(test[\"forwp\"].iloc[:hor].values, testPredict_lstm[:,1])\n",
    "    print('LSTM Test Score spot: %.5f MSE' % (testScore_spot_lstm))\n",
    "    print('LSTM Test Score forw: %.5f MSE' % (testScore_forw_lstm))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Point forecasts\n",
    "\n",
    "    # Store the original last look_back points to initiate prediction for each model\n",
    "    original_last_points = train_scal[-look_back:].reshape(1, look_back, train_scal.shape[1])\n",
    "\n",
    "    # Placeholder to store combined predictions for all horizons\n",
    "    combined_testPredict_point = np.zeros((1, hor * 2))  # Multiply by 2 as each step predicts 2 variables\n",
    "\n",
    "    for step_ahead in range(1, hor + 1):\n",
    "        # Create dataset for current horizon\n",
    "        trainX, trainY = create_dataset_point(train_scal, look_back=look_back, hor=step_ahead, exog_col=[2])\n",
    "\n",
    "        # Flatten input and output\n",
    "        trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "        trainY_flat = trainY.reshape(trainY.shape[0], -1)\n",
    "\n",
    "        # Define and fit the MLP model for the current horizon\n",
    "        model_mlp_point = Sequential()\n",
    "        model_mlp_point.add(Input(shape=(trainX_flat.shape[1],)))\n",
    "        model_mlp_point.add(Dense(32, activation='relu'))\n",
    "        model_mlp_point.add(Dense(trainY_flat.shape[1], activation=\"linear\"))\n",
    "        model_mlp_point.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model_mlp_point.fit(trainX_flat, trainY_flat, epochs=2, batch_size=1, verbose=1)\n",
    "\n",
    "        # Use the original last points to predict the step ahead\n",
    "        testX_flat = original_last_points.reshape(original_last_points.shape[0], -1)\n",
    "        testPredict_scal_flat_point = model_mlp_point.predict(testX_flat)\n",
    "        \n",
    "        # Store the prediction for the current horizon\n",
    "        combined_testPredict_point[0, (step_ahead - 1) * 2: step_ahead * 2] = testPredict_scal_flat_point\n",
    "\n",
    "    # Invert predictions\n",
    "    testPredict_scal = create_even_odd_array(combined_testPredict_point)\n",
    "    \n",
    "    # Invert predictions\n",
    "    #testPredict_mlp = scaler.inverse_transform(testPredict_scal)\n",
    "    placeholder_exog = np.zeros((testPredict_scal.shape[0], 1))  # Assuming zero as placeholder\n",
    "    testPredict_expanded = np.hstack((testPredict_scal, placeholder_exog))\n",
    "    # Step 2: Inverse transform the expanded array\n",
    "    testPredict_mlp_point_expanded = scaler.inverse_transform(testPredict_expanded)\n",
    "    # Step 3: Extract the original predictions (now inversely scaled)\n",
    "    testPredict_mlp_point = testPredict_mlp_point_expanded[:, :2]  # Assuming the first two columns are what you need\n",
    "    \n",
    "\n",
    "\n",
    "    # Reshape testPredict_mlp_point if necessary to match the test target shape and calculate error metrics\n",
    "\n",
    "    # Calculate and print MSE for each target\n",
    "    testScore_spot_mlp_point = mean_squared_error(test[\"spot\"].iloc[:hor].values, testPredict_mlp_point[:,0])\n",
    "    testScore_forw_mlp_point = mean_squared_error(test[\"forwp\"].iloc[:hor].values, testPredict_mlp_point[:,1])\n",
    "    print('MLP POINT Test Score spot: %.5f MSE' % (testScore_spot_mlp_point))\n",
    "    print('MLP POINT Test Score forw: %.5f MSE' % (testScore_forw_mlp_point))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Random Walk Predictions for comparison\n",
    "    rw_predictions = random_walk_predictions(train, test)\n",
    "    \n",
    "    # Calculate and append MSE for each model for this round\n",
    "    mse_results['MLP_spot'].append(mean_squared_error(test[\"spot\"], testPredict_mlp[:, 0]))\n",
    "    mse_results['MLP_forwp'].append(mean_squared_error(test[\"forwp\"], testPredict_mlp[:, 1]))\n",
    "    \n",
    "     # Calculate and append MSE for each model for this round\n",
    "    mse_results['MLP_POINT_spot'].append(mean_squared_error(test[\"spot\"], testPredict_mlp_point[:, 0]))\n",
    "    mse_results['MLP_POINT_forwp'].append(mean_squared_error(test[\"forwp\"], testPredict_mlp_point[:, 1]))\n",
    "\n",
    "    mse_results['LSTM_spot'].append(mean_squared_error(test[\"spot\"], testPredict_lstm[:, 0]))\n",
    "    mse_results['LSTM_forwp'].append(mean_squared_error(test[\"forwp\"], testPredict_lstm[:, 1]))\n",
    "    \n",
    "    \n",
    "\n",
    "    mse_results['RW_spot'].append(mean_squared_error(test[\"spot\"], rw_predictions[:, 0]))\n",
    "    mse_results['RW_forwp'].append(mean_squared_error(test[\"forwp\"], rw_predictions[:, 1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for MLP_spot: 1.1739799029579019\n",
      "Mean for MLP_forwp: 1.6197907482840133\n",
      "Mean for MLP_POINT_spot: 1.1914217318326712\n",
      "Mean for MLP_POINT_forwp: 1.7970986584315993\n",
      "Mean for LSTM_spot: 0.4566449139397142\n",
      "Mean for LSTM_forwp: 2.9257590007351713\n",
      "Mean for RW_spot: 0.6596570131445423\n",
      "Mean for RW_forwp: 1.0305957479552907\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
