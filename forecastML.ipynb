{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spot = pd.read_csv('./data/spot/clarkson_data.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "pmx_forw = pd.read_csv('./data/ffa/PMAX_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "csz_forw = pd.read_csv('./data/ffa/CSZ_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "smx_forw = pd.read_csv('./data/ffa/SMX_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spot</th>\n",
       "      <th>forwp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-01-04</th>\n",
       "      <td>9.815312</td>\n",
       "      <td>9.675645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-10</th>\n",
       "      <td>9.806095</td>\n",
       "      <td>9.678154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-17</th>\n",
       "      <td>9.739320</td>\n",
       "      <td>9.595263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-18</th>\n",
       "      <td>9.724361</td>\n",
       "      <td>9.587063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-19</th>\n",
       "      <td>9.710085</td>\n",
       "      <td>9.583627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                spot     forwp\n",
       "Date                          \n",
       "2006-01-04  9.815312  9.675645\n",
       "2006-01-10  9.806095  9.678154\n",
       "2006-01-17  9.739320  9.595263\n",
       "2006-01-18  9.724361  9.587063\n",
       "2006-01-19  9.710085  9.583627"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Merge data frames on the Date column\n",
    "data_combined = pd.merge(spot, smx_forw, on='Date')\n",
    "s_col = \"SMX\"\n",
    "f_col = \"1Q\"\n",
    "\n",
    "# Remove rows with NA or 0 in specific columns (assuming 'SMX' and '1Q' are column names in 'data_combined')\n",
    "data_combined = data_combined[(data_combined[s_col].notna() & data_combined[s_col] != 0) & (data_combined[f_col].notna() & data_combined[f_col] != 0)]\n",
    "\n",
    "# Transform data to log levels\n",
    "data_log_levels = pd.DataFrame()\n",
    "data_log_levels[\"spot\"] = np.log(data_combined[s_col])\n",
    "data_log_levels[\"forwp\"] = np.log(data_combined[f_col])\n",
    "data_log_levels.index = data_combined[\"Date\"]\n",
    "\n",
    "\n",
    "# Split into train and test sets\n",
    "split_index = round(len(data_log_levels) * 0.8)\n",
    "hor = 1\n",
    "train = data_log_levels.iloc[:split_index]\n",
    "test = data_log_levels.iloc[split_index:split_index+hor]\n",
    "#train.head()\n",
    "data_log_levels.head()\n",
    "train.head()\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.58835698 0.46232689]\n",
      "  [0.58560421 0.46323447]\n",
      "  [0.56566024 0.43324572]\n",
      "  ...\n",
      "  [0.52697214 0.47832482]\n",
      "  [0.53726921 0.49426967]\n",
      "  [0.55932869 0.48987549]]\n",
      "\n",
      " [[0.58560421 0.46323447]\n",
      "  [0.56566024 0.43324572]\n",
      "  [0.56119228 0.43027925]\n",
      "  ...\n",
      "  [0.53726921 0.49426967]\n",
      "  [0.55932869 0.48987549]\n",
      "  [0.57578165 0.47653813]]\n",
      "\n",
      " [[0.56566024 0.43324572]\n",
      "  [0.56119228 0.43027925]\n",
      "  [0.55692839 0.42903601]\n",
      "  ...\n",
      "  [0.55932869 0.48987549]\n",
      "  [0.57578165 0.47653813]\n",
      "  [0.57593472 0.47914982]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.23767389 0.24156131]\n",
      "  [0.26081647 0.25566866]\n",
      "  [0.29532163 0.27835084]\n",
      "  ...\n",
      "  [0.33022471 0.28575427]\n",
      "  [0.35905714 0.29308177]\n",
      "  [0.38224381 0.29257365]]\n",
      "\n",
      " [[0.26081647 0.25566866]\n",
      "  [0.29532163 0.27835084]\n",
      "  [0.30663397 0.27782158]\n",
      "  ...\n",
      "  [0.35905714 0.29308177]\n",
      "  [0.38224381 0.29257365]\n",
      "  [0.39615883 0.29126376]]\n",
      "\n",
      " [[0.29532163 0.27835084]\n",
      "  [0.30663397 0.27782158]\n",
      "  [0.31079384 0.27713997]\n",
      "  ...\n",
      "  [0.38224381 0.29257365]\n",
      "  [0.39615883 0.29126376]\n",
      "  [0.40421298 0.28693668]]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming data_combined is your final DataFrame after preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "train_scal = scaler.fit_transform(train)\n",
    "\n",
    "# Convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=10, is_test=False):\n",
    "    X, Y = [], []\n",
    "    if is_test:  # for test data, we just need the last entry for 1-step ahead forecast\n",
    "        X.append(dataset[-look_back:])\n",
    "        return np.array(X), None\n",
    "    else:\n",
    "        for i in range(look_back, len(dataset)):\n",
    "            X.append(dataset[i-look_back:i])\n",
    "            Y.append(dataset[i])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 10  # Adjust based on your temporal structure\n",
    "trainX, trainY = create_dataset(train_scal, look_back)\n",
    "print(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mariumbo\\master-thesis\\env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "897/897 - 2s - 2ms/step - loss: 0.0073\n",
      "Epoch 2/10\n",
      "897/897 - 1s - 963us/step - loss: 0.0011\n",
      "Epoch 3/10\n",
      "897/897 - 1s - 951us/step - loss: 6.2820e-04\n",
      "Epoch 4/10\n",
      "897/897 - 1s - 965us/step - loss: 5.3665e-04\n",
      "Epoch 5/10\n",
      "897/897 - 1s - 945us/step - loss: 4.7060e-04\n",
      "Epoch 6/10\n",
      "897/897 - 1s - 945us/step - loss: 4.8689e-04\n",
      "Epoch 7/10\n",
      "897/897 - 1s - 944us/step - loss: 4.3160e-04\n",
      "Epoch 8/10\n",
      "897/897 - 1s - 945us/step - loss: 4.0039e-04\n",
      "Epoch 9/10\n",
      "897/897 - 1s - 945us/step - loss: 3.8869e-04\n",
      "Epoch 10/10\n",
      "897/897 - 1s - 945us/step - loss: 4.2340e-04\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and fit the MLP model\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "f = trainX_flat.shape[1]\n",
    "\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(32, input_dim=trainX_flat.shape[1], activation='relu'))\n",
    "model_mlp.add(Dense(16, activation=\"relu\"))\n",
    "model_mlp.add(Dense(2, activation=\"linear\"))\n",
    "model_mlp.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_mlp.fit(trainX_flat, trainY, epochs=10, batch_size=2, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "trainPredict_scal = model_mlp.predict(trainX_flat)\n",
    "testX = create_dataset(trainX[-look_back:])\n",
    "testX_flat = testX.reshape(trainX.shape[0], -1)\n",
    "testPredict_scal = model_mlp.predict(testX_flat)\n",
    "\n",
    "# Invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict_scal)\n",
    "testPredict = scaler.inverse_transform(testPredict_scal)\n",
    "\n",
    "# Calculate mean squared error\n",
    "trainScore = mean_squared_error(trainY, trainPredict[:,0])\n",
    "testScore = mean_squared_error(testY, testPredict[:,0])\n",
    "print('Train Score: %.2f MSE' % (trainScore))\n",
    "print('Test Score: %.2f MSE' % (testScore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "print(trainX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
