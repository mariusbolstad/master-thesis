{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/8c6y7sz92fbdm4xspk2650lm0000gn/T/ipykernel_37974/748165724.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spot = pd.read_csv('./data/spot/clarkson_data.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "pmx_forw = pd.read_csv('./data/ffa/PMAX_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "csz_forw = pd.read_csv('./data/ffa/CSZ_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n",
    "smx_forw = pd.read_csv('./data/ffa/SMX_FFA.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./env/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.2-cp39-cp39-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n",
      "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.0 scikit-learn-1.4.2 scipy-1.13.0 threadpoolctl-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[[0.58835698 0.46232689]\n",
      " [0.58560421 0.46323447]\n",
      " [0.56566024 0.43324572]\n",
      " ...\n",
      " [0.39615883 0.29126376]\n",
      " [0.40421298 0.28693668]\n",
      " [0.40808548 0.2882623 ]]\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Merge data frames on the Date column\n",
    "data_combined = pd.merge(spot, smx_forw, on='Date')\n",
    "s_col = \"SMX\"\n",
    "f_col = \"1Q\"\n",
    "\n",
    "# Remove rows with NA or 0 in specific columns (assuming 'SMX' and '1Q' are column names in 'data_combined')\n",
    "data_combined = data_combined[(data_combined[s_col].notna() & data_combined[s_col] != 0) & (data_combined[f_col].notna() & data_combined[f_col] != 0)]\n",
    "\n",
    "# Transform data to log levels\n",
    "data_log_levels = pd.DataFrame()\n",
    "data_log_levels[\"spot\"] = np.log(data_combined[s_col])\n",
    "data_log_levels[\"forwp\"] = np.log(data_combined[f_col])\n",
    "data_log_levels.index = data_combined[\"Date\"]\n",
    "\n",
    "\n",
    "# Split into train and test sets\n",
    "split_index = round(len(data_log_levels) * 0.8)\n",
    "hor = 30\n",
    "train = data_log_levels.iloc[:split_index]\n",
    "test = data_log_levels.iloc[split_index:split_index+hor]\n",
    "#train.head()\n",
    "data_log_levels.head()\n",
    "train.head()\n",
    "#test.head()\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "train_scal = scaler.fit_transform(train)\n",
    "print(train_scal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.58835698 0.46232689]\n",
      "  [0.58560421 0.46323447]\n",
      "  [0.56566024 0.43324572]\n",
      "  ...\n",
      "  [0.58151033 0.40589457]\n",
      "  [0.58443316 0.41507072]\n",
      "  [0.59184327 0.43141927]]\n",
      "\n",
      " [[0.58560421 0.46323447]\n",
      "  [0.56566024 0.43324572]\n",
      "  [0.56119228 0.43027925]\n",
      "  ...\n",
      "  [0.58443316 0.41507072]\n",
      "  [0.59184327 0.43141927]\n",
      "  [0.59970273 0.44814931]]\n",
      "\n",
      " [[0.56566024 0.43324572]\n",
      "  [0.56119228 0.43027925]\n",
      "  [0.55692839 0.42903601]\n",
      "  ...\n",
      "  [0.59184327 0.43141927]\n",
      "  [0.59970273 0.44814931]\n",
      "  [0.60193977 0.43988099]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.20266659 0.24306369]\n",
      "  [0.19326445 0.25687326]\n",
      "  [0.19606777 0.2571137 ]\n",
      "  ...\n",
      "  [0.27301676 0.10205481]\n",
      "  [0.23433078 0.21148978]\n",
      "  [0.22068284 0.2137549 ]]\n",
      "\n",
      " [[0.19326445 0.25687326]\n",
      "  [0.19606777 0.2571137 ]\n",
      "  [0.20555977 0.26527338]\n",
      "  ...\n",
      "  [0.23433078 0.21148978]\n",
      "  [0.22068284 0.2137549 ]\n",
      "  [0.20941293 0.20884436]]\n",
      "\n",
      " [[0.19606777 0.2571137 ]\n",
      "  [0.20555977 0.26527338]\n",
      "  [0.2159535  0.27171733]\n",
      "  ...\n",
      "  [0.22068284 0.2137549 ]\n",
      "  [0.20941293 0.20884436]\n",
      "  [0.20302236 0.20144473]]]\n"
     ]
    }
   ],
   "source": [
    "# Convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=10, is_test=False):\n",
    "    X, Y = [], []\n",
    "    if is_test:  # for test data, we just need the last entry for 1-step ahead forecast\n",
    "        X = dataset[-1:,:,]\n",
    "        return X, None\n",
    "    else:\n",
    "        for i in range(look_back, len(dataset)- hor + 1):\n",
    "            X.append(dataset[i-look_back:i])\n",
    "            Y.append(dataset[i:i+hor])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 20  # Adjust based on your temporal structure\n",
    "trainX, trainY = create_dataset(train_scal, look_back)\n",
    "print(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "877/877 - 1s - 918us/step - loss: 0.0154\n",
      "Epoch 2/10\n",
      "877/877 - 0s - 495us/step - loss: 0.0077\n",
      "Epoch 3/10\n",
      "877/877 - 0s - 514us/step - loss: 0.0072\n",
      "Epoch 4/10\n",
      "877/877 - 0s - 500us/step - loss: 0.0069\n",
      "Epoch 5/10\n",
      "877/877 - 0s - 448us/step - loss: 0.0067\n",
      "Epoch 6/10\n",
      "877/877 - 0s - 456us/step - loss: 0.0064\n",
      "Epoch 7/10\n",
      "877/877 - 0s - 457us/step - loss: 0.0064\n",
      "Epoch 8/10\n",
      "877/877 - 0s - 459us/step - loss: 0.0064\n",
      "Epoch 9/10\n",
      "877/877 - 0s - 450us/step - loss: 0.0062\n",
      "Epoch 10/10\n",
      "877/877 - 0s - 438us/step - loss: 0.0060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1577aaa9ed0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the MLP model\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "trainY_flat = trainY.reshape(trainY.shape[0], -1)\n",
    "\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(32, input_dim=trainX_flat.shape[1], activation='relu'))\n",
    "model_mlp.add(Dense(trainY_flat.shape[1], activation=\"linear\"))\n",
    "model_mlp.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_mlp.fit(trainX_flat, trainY_flat, epochs=10, batch_size=2, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "[8.567407  8.517986  8.631262  8.551814  8.521142  8.559909  8.531753\n",
      " 8.675044  8.672477  8.712001  8.725069  8.655896  8.654268  8.75828\n",
      " 8.646091  8.820569  8.807465  8.7721405 8.753044  8.802827  8.735529\n",
      " 8.783503  8.739636  8.785037  8.70078   8.697184  8.804268  8.809682\n",
      " 8.716883  8.778508 ]\n",
      "Test Score spot: 0.31128 MSE\n",
      "Test Score forw: 0.13583 MSE\n",
      "Random Walk Test Score spot: 0.00323 MSE\n",
      "Random Walk Test Score forw: 0.01074 MSE\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "trainPredict_scal_flat = model_mlp.predict(trainX_flat)\n",
    "\n",
    "\n",
    "def create_even_odd_array(arr):\n",
    "    \"\"\"\n",
    "    Returns an array where the first column contains values from even positions\n",
    "    and the second column contains values from odd positions of the original array.\n",
    "    \"\"\"\n",
    "    return arr.reshape(-1, 2)\n",
    "\n",
    "\n",
    "\n",
    "testX, _ = create_dataset(trainX, look_back=look_back, is_test=True)\n",
    "testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "testPredict_scal_flat = model_mlp.predict(testX_flat)\n",
    "testPredict_scal = create_even_odd_array(testPredict_scal_flat)\n",
    "\n",
    "\n",
    "# Invert predictions\n",
    "#trainPredict = scaler.inverse_transform(trainPredict_scal)\n",
    "testPredict = scaler.inverse_transform(testPredict_scal)\n",
    "print(testPredict[:, 0])\n",
    "\n",
    "# Calculate mean squared error\n",
    "testScore = mean_squared_error(test[\"spot\"], testPredict[:,0])\n",
    "testScoreForw = mean_squared_error(test[\"forwp\"], testPredict[:,1])\n",
    "print('Test Score spot: %.5f MSE' % (testScore))\n",
    "print('Test Score forw: %.5f MSE' % (testScoreForw))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_walk_predictions(training_data, testing_data):\n",
    "    \"\"\"\n",
    "    Generates Random Walk predictions where the next value is assumed to be the last observed value.\n",
    "    \n",
    "    Parameters:\n",
    "    - training_data: DataFrame containing the training data.\n",
    "    - testing_data: DataFrame containing the test data.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: Numpy array containing Random Walk predictions for the test set.\n",
    "    \"\"\"\n",
    "    # Last observed values from the training set\n",
    "    last_observed_spot = training_data['spot'].iloc[-1]\n",
    "    last_observed_forwp = training_data['forwp'].iloc[-1]\n",
    "    \n",
    "    # Create an array of predictions, each one equal to the last observed values\n",
    "    predictions = np.tile([last_observed_spot, last_observed_forwp], (len(testing_data), 1))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Generate Random Walk predictions for the test set\n",
    "rw_predictions = random_walk_predictions(train, test)\n",
    "\n",
    "# Benchmark Random Walk model by calculating the MSE\n",
    "rw_testScore_spot = mean_squared_error(test[\"spot\"].values, rw_predictions[:, 0])\n",
    "rw_testScore_forw = mean_squared_error(test[\"forwp\"].values, rw_predictions[:, 1])\n",
    "\n",
    "print('Random Walk Test Score spot: %.5f MSE' % (rw_testScore_spot))\n",
    "print('Random Walk Test Score forw: %.5f MSE' % (rw_testScore_forw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 - 2s - 39ms/step - loss: 0.0655\n",
      "Epoch 2/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0106\n",
      "Epoch 3/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0091\n",
      "Epoch 4/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0088\n",
      "Epoch 5/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0087\n",
      "Epoch 6/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0080\n",
      "Epoch 7/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0081\n",
      "Epoch 8/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0074\n",
      "Epoch 9/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0074\n",
      "Epoch 10/10\n",
      "55/55 - 0s - 6ms/step - loss: 0.0075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15709e45ed0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model_lstm.add(LSTM(units=50))\n",
    "model_lstm.add(Dense(trainY_flat.shape[1], activation='linear'))  # Assuming multi-step forecasting\n",
    "\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.fit(trainX, trainY_flat, epochs=10, batch_size=32, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "LSTM Test Score spot: 0.03600 MSE\n",
      "LSTM Test Score forw: 0.03061 MSE\n"
     ]
    }
   ],
   "source": [
    "# Prepare the last sequence from the training set as the input for the first prediction\n",
    "testX_last_sequence = train_scal[-look_back:].reshape(1, look_back, train_scal.shape[1])\n",
    "\n",
    "# Make predictions\n",
    "testPredict_scal_flat = model_lstm.predict(testX_last_sequence)\n",
    "\n",
    "# Since you're predicting `hor` steps ahead, you might need to adjust the code to generate\n",
    "# multiple steps if your LSTM model is set up for single-step predictions.\n",
    "# For simplicity, this example directly uses the LSTM output for multi-step predictions.\n",
    "\n",
    "# Invert scaling\n",
    "testPredict_scal = create_even_odd_array(testPredict_scal_flat)\n",
    "testPredict = scaler.inverse_transform(testPredict_scal)\n",
    "\n",
    "\n",
    "# Calculate and print MSE for each target\n",
    "testScore_spot_lstm = mean_squared_error(test[\"spot\"].iloc[:hor].values, testPredict[:,0])\n",
    "testScore_forw_lstm = mean_squared_error(test[\"forwp\"].iloc[:hor].values, testPredict[:,1])\n",
    "print('LSTM Test Score spot: %.5f MSE' % (testScore_spot_lstm))\n",
    "print('LSTM Test Score forw: %.5f MSE' % (testScore_forw_lstm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mariumbo\\master-thesis\\env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - loss: 0.0494\n",
      "Epoch 2/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step - loss: 0.0020\n",
      "Epoch 3/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461us/step - loss: 0.0014\n",
      "Epoch 4/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461us/step - loss: 0.0014\n",
      "Epoch 5/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 457us/step - loss: 0.0014\n",
      "Epoch 6/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step - loss: 0.0012\n",
      "Epoch 7/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 449us/step - loss: 0.0013\n",
      "Epoch 8/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step - loss: 0.0013\n",
      "Epoch 9/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 491us/step - loss: 0.0012\n",
      "Epoch 10/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step - loss: 0.0011\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m testPredict_mlp \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(testPredict_scal)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Calculate mean squared error\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m testScore \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestPredict_mlp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m testScoreForw \u001b[38;5;241m=\u001b[39m mean_squared_error(test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforwp\u001b[39m\u001b[38;5;124m\"\u001b[39m], testPredict_mlp[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Score spot MLP: \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m MSE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (testScore))\n",
      "File \u001b[1;32mc:\\Users\\mariumbo\\master-thesis\\env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mariumbo\\master-thesis\\env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    495\u001b[0m         )\n\u001b[1;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\mariumbo\\master-thesis\\env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    104\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\mariumbo\\master-thesis\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 5]"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Number of rounds based on the test set size and forecast horizon\n",
    "num_rounds =  3  # Adjusted to ensure we don't exceed the test set\n",
    "look_back = 10  # Adjust based on your temporal structure\n",
    "hor = 5\n",
    "\n",
    "# Initialize dictionary to store MSE results for each model\n",
    "mse_results = {\n",
    "    'MLP_spot': [],\n",
    "    'MLP_forwp': [],\n",
    "    'LSTM_spot': [],\n",
    "    'LSTM_forwp': [],\n",
    "    'RW_spot': [],\n",
    "    'RW_forwp': [],\n",
    "}\n",
    "\n",
    "\n",
    "def create_even_odd_array(arr):\n",
    "    \"\"\"\n",
    "    Returns an array where the first column contains values from even positions\n",
    "    and the second column contains values from odd positions of the original array.\n",
    "    \"\"\"\n",
    "    return arr.reshape(-1, 2)\n",
    "\n",
    "\n",
    "# Adjust train and test sets for each forecast round\n",
    "for round in range(1, num_rounds + 1):\n",
    "    print(\"Round\", round)\n",
    "    # Define new split point for each round\n",
    "    split_index = split_index + (round - 1) * hor\n",
    "    \n",
    "    # Update train and test sets\n",
    "    train = data_log_levels.iloc[:split_index]\n",
    "    test = data_log_levels.iloc[split_index:split_index+hor]\n",
    "\n",
    "    #Scale train set\n",
    "    train_scal = scaler.fit_transform(train)\n",
    "\n",
    "    trainX, trainY = create_dataset(train_scal, look_back)\n",
    "    # Create and fit the MLP model\n",
    "\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    trainY_flat = trainY.reshape(trainY.shape[0], -1)\n",
    "\n",
    "    model_mlp = Sequential()\n",
    "    model_mlp.add(Dense(32, input_dim=trainX_flat.shape[1], activation='relu'))\n",
    "    model_mlp.add(Dense(trainY_flat.shape[1], activation=\"linear\"))\n",
    "    model_mlp.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_mlp.fit(trainX_flat, trainY_flat, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "\n",
    "    # Make predictions\n",
    "    trainPredict_scal_flat = model_mlp.predict(trainX_flat)\n",
    "    testX = train_scal[-look_back:].reshape(1, look_back, train_scal.shape[1])\n",
    "    #testX, _ = create_dataset(trainX, look_back=look_back, is_test=True)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "    testPredict_scal_flat = model_mlp.predict(testX_flat)\n",
    "    testPredict_scal = create_even_odd_array(testPredict_scal_flat)\n",
    "\n",
    "    # Invert predictions\n",
    "    testPredict_mlp = scaler.inverse_transform(testPredict_scal)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    testScore = mean_squared_error(test[\"spot\"], testPredict_mlp[:,0])\n",
    "    testScoreForw = mean_squared_error(test[\"forwp\"], testPredict_mlp[:,1])\n",
    "    print('Test Score spot MLP: %.5f MSE' % (testScore))\n",
    "    print('Test Score forw MLP: %.5f MSE' % (testScoreForw))\n",
    "\n",
    "\n",
    "\n",
    "    # Define the LSTM model\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model_lstm.add(LSTM(units=50))\n",
    "    model_lstm.add(Dense(trainY_flat.shape[1], activation='linear'))  # Assuming multi-step forecasting\n",
    "\n",
    "    model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_lstm.fit(trainX, trainY_flat, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "    # Prepare the last sequence from the training set as the input for the first prediction\n",
    "    #testX_last_sequence = train_scal[-look_back:].reshape(1, look_back, train_scal.shape[1])\n",
    "\n",
    "    # Make predictions\n",
    "    testPredict_scal_flat = model_lstm.predict(testX)\n",
    "\n",
    "    # Since you're predicting `hor` steps ahead, you might need to adjust the code to generate\n",
    "    # multiple steps if your LSTM model is set up for single-step predictions.\n",
    "    # For simplicity, this example directly uses the LSTM output for multi-step predictions.\n",
    "\n",
    "    # Invert scaling\n",
    "    testPredict_scal = create_even_odd_array(testPredict_scal_flat)\n",
    "    testPredict_lstm = scaler.inverse_transform(testPredict_scal)\n",
    "\n",
    "\n",
    "    # Calculate and print MSE for each target\n",
    "    testScore_spot_lstm = mean_squared_error(test[\"spot\"].iloc[:hor].values, testPredict_lstm[:,0])\n",
    "    testScore_forw_lstm = mean_squared_error(test[\"forwp\"].iloc[:hor].values, testPredict_lstm[:,1])\n",
    "    print('LSTM Test Score spot: %.5f MSE' % (testScore_spot_lstm))\n",
    "    print('LSTM Test Score forw: %.5f MSE' % (testScore_forw_lstm))\n",
    "\n",
    "\n",
    "    # Random Walk Predictions for comparison\n",
    "    rw_predictions = random_walk_predictions(train, test)\n",
    "    \n",
    "    # Calculate and append MSE for each model for this round\n",
    "    mse_results['MLP_spot'].append(mean_squared_error(test[\"spot\"], testPredict_mlp[:, 0]))\n",
    "    mse_results['MLP_forwp'].append(mean_squared_error(test[\"forwp\"], testPredict_mlp[:, 1]))\n",
    "\n",
    "    mse_results['LSTM_spot'].append(mean_squared_error(test[\"spot\"], testPredict_lstm[:, 0]))\n",
    "    mse_results['LSTM_forwp'].append(mean_squared_error(test[\"forwp\"], testPredict_lstm[:, 1]))\n",
    "\n",
    "    mse_results['RW_spot'].append(mean_squared_error(test[\"spot\"], rw_predictions[:, 0]))\n",
    "    mse_results['RW_forwp'].append(mean_squared_error(test[\"forwp\"], rw_predictions[:, 1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in mse_results.items():\n",
    "    mean = sum(values) / len(values) * 100\n",
    "    print(f\"Mean for {key}: {mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
